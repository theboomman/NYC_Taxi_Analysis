{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d69084b7-5c88-47e8-b815-db7be71e4ea5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7. Combine the 2 type of taxi dataframe in one dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea34a07-3714-42c2-b837-8686f5447e0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load the cleaned sampled yellow taxi dataframe\n",
    "df_y_s_cleaned = spark.read.option(\"header\", True).parquet(\"/FileStore/tables/df_yellow_cleaned.parquet\")\n",
    "display(df_y_s_cleaned)\n",
    "df_y_s_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd737299-c57d-4594-8e55-ed5e7dfaaa2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load the cleaned green taxi dataframe\n",
    "df_g_cleaned = spark.read.option(\"header\", True).parquet(\"/FileStore/tables/df_green_cleaned.parquet\")\n",
    "display(df_g_cleaned)\n",
    "df_g_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d3e311-fa43-4fdd-8f67-c730740c00aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create the new column to identify which type of NYC taxi yellow = 1 and green = 2\n",
    "from pyspark.sql.functions import lit\n",
    "df_y_s_cleaned = df_y_s_cleaned.withColumn(\"taxi_color\", lit(1))\n",
    "display(df_y_s_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19d496d1-6454-4214-9dd9-077c60be6e20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Do the same to green taxi dataframe\n",
    "df_g_cleaned = df_g_cleaned.withColumn(\"taxi_color\", lit(2))\n",
    "display(df_g_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef956381-561c-4e76-8fbe-b4d64f7c0c1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Combine 2 datasets together.\n",
    "df_combined = df_y_s_cleaned.unionByName(df_g_cleaned, allowMissingColumns=True)\n",
    "display(df_combined)\n",
    "df_combined.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eea592a-742b-4048-aaf1-aebe585a7c8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "8. Combine with Location dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b622164a-bc41-4a0a-aed3-71cd8115f590",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load the loaction dataframe\n",
    "df_l = spark.read.option(\"header\", True).csv(\"/FileStore/tables/taxi_zone_lookup.csv\")\n",
    "display(df_l)\n",
    "df_l.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67bf8a7e-b7a8-4caf-941d-5c2eb129ecce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#find any missing data\n",
    "from pyspark.sql.functions import col \n",
    "df_combined.select(\"PULocationID\", \"DOLocationID\") \\\n",
    "    .where((col(\"PULocationID\").isNull()) | (col(\"DOLocationID\").isNull())) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bab4e1f8-fa7f-4348-8dd1-1aa60f91bbd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Clone df_l into 2 dataframes and change the column name to make them be able to join the 1 table\n",
    "df_l_pu = df_l.withColumnRenamed(\"LocationID\", \"PULocationID\").withColumnRenamed(\"Zone\", \"Zone_Pickup\").withColumnRenamed(\"Borough\", \"Borough_Pickup\").withColumnRenamed(\"service_zone\", \"service_zone_pickup\")\n",
    "df_l_do = df_l.withColumnRenamed(\"LocationID\", \"DOLocationID\").withColumnRenamed(\"Zone\", \"Zone_Dropoff\").withColumnRenamed(\"Borough\", \"Borough_Dropoff\").withColumnRenamed(\"service_zone\", \"service_zone_dropoff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59fe957-3a3a-4717-836b-f9c11f6577fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Show the location tables\n",
    "print(\"Pick Up Location\")\n",
    "df_l_pu.show()\n",
    "\n",
    "print(\"Drop Off Location\")\n",
    "df_l_do.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65581df6-3ffd-46db-a258-1da7f7e29b6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join for Pickup Location \n",
    "df_combined_n = df_combined.join(df_l_pu, \"PULocationID\", how=\"left\")\n",
    "display(df_combined_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d358e0-7dce-4b9c-81b6-f5d9347af8e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join for Dropoff Location \n",
    "df_combined_n = df_combined_n.join(df_l_do, \"DOLocationID\", how=\"left\")\n",
    "display(df_combined_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c1dbb4-d038-46fe-a0ab-425dbcf1f2c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Count row to make sure there is no row missing.\n",
    "df_combined_n.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d194baba-f5eb-4c60-9644-246f2917bbee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Save the whole dataset\n",
    "df_combined_n.write.mode(\"overwrite\").parquet(\"/FileStore/tables/df_final.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cabf20ed-e3bf-4e66-8d0e-af99fe3f0f56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = spark.read.option(\"header\", True).parquet(\"/FileStore/tables/df_final.parquet\")\n",
    "display(df_final)\n",
    "df_final.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "178234b2-1b47-4103-9151-89517fa69cf3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Part 2 - Business Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec8473ba-c06f-478f-986f-2cdad7a85c7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Use SQL spark \n",
    "df_final.createOrReplaceTempView(\"taxi_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8f02b40-21ab-401a-be6c-e7685b4e25c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Show the whole dataset\n",
    "df = spark.sql(\"SELECT * FROM taxi_trips LIMIT 5\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce241f4-30d5-468d-85b0-e899004fc970",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. For each year and month (e.g January 2020 => “2020-01-01” or “2020-01” or “Jan 2020\"\n",
    "# a. What was the total number of trips?\n",
    "# b. Which day of week (e.g. monday, tuesday, etc..) had the most trips?\n",
    "# c. Which hour of the day had the most trips?\n",
    "# d. What was the average number of passengers?\n",
    "# e. What was the average amount paid per trip (using total_amount)?\n",
    "# f. What was the average amount paid per passenger (using total_amount)?\n",
    "answer_1 = spark.sql(\"\"\"\n",
    "WITH trips_year_month AS (\n",
    "    SELECT \n",
    "        DATE_FORMAT(d_pickup, 'yyyy-MM') AS year_month,\n",
    "        DATE_FORMAT(d_pickup, 'EEEE') AS day_of_week,\n",
    "        HOUR(t_pickup) AS hour_of_day,\n",
    "        COUNT(*) AS total_trips,\n",
    "        AVG(passenger_count) AS avg_passenger,\n",
    "        AVG(total_amount) AS avg_paid_per_trip,\n",
    "        AVG(total_amount / passenger_count) AS avg_paid_per_passenger\n",
    "    FROM taxi_trips\n",
    "    GROUP BY year_month, day_of_week, hour_of_day\n",
    "),\n",
    "ranked_d_hr AS (\n",
    "    SELECT *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY year_month, day_of_week ORDER BY total_trips DESC) AS rank_d,\n",
    "        ROW_NUMBER() OVER (PARTITION BY year_month, hour_of_day ORDER BY total_trips DESC) AS rank_hr,\n",
    "        SUM(total_trips) OVER (PARTITION BY year_month ) AS month_total_trips\n",
    "    FROM trips_year_month\n",
    ")\n",
    "SELECT\n",
    "    year_month,\n",
    "    FIRST(month_total_trips) AS month_total_trips,\n",
    "    FIRST(day_of_week) AS most_trips_day,\n",
    "    FIRST(hour_of_day) AS most_trips_hour,\n",
    "    ROUND(AVG(avg_passenger), 2) AS avg_passenger,\n",
    "    ROUND(AVG(avg_paid_per_trip), 2) AS avg_paid_per_trip,\n",
    "    ROUND(AVG(avg_paid_per_passenger), 2) AS avg_paid_per_passenger\n",
    "FROM ranked_d_hr\n",
    "WHERE rank_d = 1 AND rank_hr = 1\n",
    "GROUP BY year_month\n",
    "ORDER BY year_month\n",
    ";\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "display(answer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa6c2d14-8d3e-4ab3-befd-91a938324984",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. For each taxi colour (yellow and green):\n",
    "# a. What was the average, median, minimum and maximum trip duration in minutes (with 2 decimals, eg. 90 seconds = 1.50 min)?\n",
    "# b. What was the average, median, minimum and maximum trip distance in km?\n",
    "# c. What was the average, median, minimum and maximum speed in km per hour?\n",
    "answer_2 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    taxi_color,\n",
    "    ROUND(AVG(duration_hours * 60), 2) AS avg_duration_min,             --a. Duration_hours of trip convert into minute\n",
    "    ROUND(PERCENTILE_APPROX(duration_hours * 60, 0.5), 2) AS med_duration_min,\n",
    "    ROUND(MIN(duration_hours * 60), 2) AS min_duration_min,\n",
    "    ROUND(MAX(duration_hours * 60), 2) AS max_duration_min,\n",
    "    \n",
    "    ROUND(AVG(trip_distance * 1.61), 2) AS avg_trip_distance_km,        --b. Trip_distance convert into kilometre\n",
    "    ROUND(PERCENTILE_APPROX(trip_distance * 1.61, 0.5), 2) AS med_trip_distance_km,\n",
    "    ROUND(MIN(trip_distance * 1.61), 2) AS min_trip_distance_km,\n",
    "    ROUND(MAX(trip_distance * 1.61), 2) AS max_trip_distance_km,\n",
    "    ROUND(AVG(trip_speed * 1.61), 2) AS avg_trip_speed_km_hr,\n",
    "\n",
    "    ROUND(PERCENTILE_APPROX(trip_speed * 1.61, 0.5), 2) AS med_trip_speed_km_hr,    --c. trip_speed convert into km/hr\n",
    "    ROUND(MIN(trip_speed * 1.61), 2) AS min_trip_speed_km_hr,\n",
    "    ROUND(MAX(trip_speed * 1.61), 2) AS max_trip_speed_km_hr\n",
    "FROM\n",
    "    taxi_trips\n",
    "GROUP BY taxi_color\n",
    "ORDER BY taxi_color\n",
    ";\n",
    "\"\"\")\n",
    "display(answer_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60d8189-9e6f-485e-a75e-634b0667b305",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. For each taxi colour (yellow and green), each pair of pick up and drop off locations\n",
    "# (use boroughs not the id), each month, each day of week and each hours:\n",
    "# a. What was the total number of trips?\n",
    "# b. What was the average distance?\n",
    "# c. What was the average amount paid per trip (using total_amount)?\n",
    "# d. What was the total amount paid (using total_amount)?\n",
    "\n",
    "answer_3 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    taxi_color,\n",
    "    borough_pickup,\n",
    "    borough_dropoff,\n",
    "    MONTH(d_pickup) AS month,\n",
    "    DATE_FORMAT(d_pickup, 'EEEE') AS day_of_week,\n",
    "    HOUR(t_pickup) AS hour,\n",
    "    COUNT(*) AS total_trips,\n",
    "    ROUND(AVG(trip_distance), 2) AS avg_distance,\n",
    "    ROUND(AVG(total_amount), 2) AS avg_paid_per_trip,\n",
    "    ROUND(SUM(total_amount), 2) AS total_amount_paid\n",
    "FROM \n",
    "    taxi_trips\n",
    "GROUP BY\n",
    "    taxi_color,\n",
    "    borough_pickup,\n",
    "    borough_dropoff,\n",
    "    month,\n",
    "    day_of_week,\n",
    "    hour\n",
    "\n",
    "ORDER BY\n",
    "    taxi_color,\n",
    "    month,\n",
    "    day_of_week,\n",
    "    hour\n",
    ";\n",
    "\"\"\")\n",
    "display(answer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3cfb5f-a508-4965-b98a-1ca0104c04df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. What was the percentage of trips where drivers received tips?\n",
    "answer_4 = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    SUM(CASE WHEN tip_amount > 0 THEN 1 END) AS number_tipped_trips,\n",
    "    ROUND((number_tipped_trips / COUNT(*)) *100, 2) AS percentage_of_tipped_trip\n",
    "FROM taxi_trips;\n",
    ";\n",
    "\"\"\")\n",
    "display(answer_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b8aa31-10e9-4ceb-ad2a-09315ac08097",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5. For trips where the driver received tips, what was the percentage where the driver received tips of at least $5?\n",
    "answer_5 = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    SUM(CASE WHEN tip_amount >= 5 THEN 1 END) AS number_tipped_trips_at_least_5,\n",
    "    ROUND((number_tipped_trips_at_least_5 / COUNT(*)) *100, 2) AS percentage_of_tipped_trip_at_least_5\n",
    "FROM \n",
    "    taxi_trips\n",
    "WHERE \n",
    "    tip_amount > 0\n",
    ";\n",
    "\"\"\")\n",
    "display(answer_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "241cab54-1088-461a-8f07-61f92b47133b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Classify each trip into bins of durations:\n",
    "# a. Under 5 Mins\n",
    "# b. From 5 mins to 10 mins\n",
    "# c. From 10 mins to 20 mins\n",
    "# d. From 20 mins to 30 mins\n",
    "# e. From 30 mins to 60 mins\n",
    "# f. At least 60 mins\n",
    "# Then for each bins, calculate:\n",
    "# a. Average speed (km per hour)\n",
    "# b. Average distance per dollar (km per $)\n",
    "answer_6 = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    bins_duration,\n",
    "    ROUND(AVG(avg_trip_speed_kmhr), 2) AS avg_trip_speed_kmhr,\n",
    "    ROUND(AVG(avg_distance_km_per_dollar), 2) AS avg_distance_km_per_dollar\n",
    "FROM (\n",
    "    SELECT\n",
    "        (duration_hours * 60) AS duration_mins,\n",
    "        CASE \n",
    "            WHEN duration_mins < 5 THEN '1) Under 5 Mins'                            -- Under 5 mins\n",
    "            WHEN duration_mins >= 5 AND duration_mins < 10 THEN '2) 5 to 10 Mins'    -- 5 mins to 10 mins\n",
    "            WHEN duration_mins >= 10 AND duration_mins < 20 THEN '3) 10 to 20 Mins'  -- 10 mins to 20 mins\n",
    "            WHEN duration_mins >= 20 AND duration_mins < 30 THEN '4) 20 to 30 Mins'  -- 20 mins to 30 mins\n",
    "            WHEN duration_mins >= 30 AND duration_mins < 60 THEN '5) 30 to 60 Mins'  -- 30 mins to 60 mins\n",
    "            ELSE '6) At least 60 Mins'                                               -- At least 60 mins\n",
    "        END AS bins_duration,\n",
    "        ROUND(trip_speed * 1.61, 2) AS avg_trip_speed_kmhr,                  -- Convert trip_speed to km/hr\n",
    "        ROUND((trip_distance * 1.61) / total_amount, 2) AS avg_distance_km_per_dollar    -- Average distance per dollar\n",
    "    FROM \n",
    "        taxi_trips\n",
    ") AS sub_query\n",
    "GROUP BY\n",
    "    bins_duration\n",
    "ORDER BY\n",
    "    bins_duration;\n",
    "\n",
    "\"\"\")\n",
    "display(answer_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe0af6a-06cd-4f53-b1d1-e709f056a61e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7. Which duration bin will you advise a taxi driver to target to maximise his income?\n",
    "# Calculate the average fare per trip, average distance per dollar, Total trips in each bin and total time spent \n",
    "answer_7 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    bins_duration,\n",
    "    ROUND(AVG(total_amount), 2) AS avg_paid_per_trip,         -- Average fare per trip\n",
    "    ROUND(AVG((trip_distance * 1.61) / total_amount), 2) AS avg_distance_per_dollar, -- Average distance per dollar\n",
    "    COUNT(*) AS total_trips,                                  -- Total number of trips in each bin\n",
    "    ROUND(SUM(duration_hours * 60), 2) AS total_duration_mins -- Total time spent on trips\n",
    "FROM (\n",
    "    SELECT\n",
    "        duration_hours * 60 AS duration_mins,\n",
    "        CASE \n",
    "            WHEN duration_mins < 5 THEN '1) Under 5 Mins'\n",
    "            WHEN duration_mins >= 5 AND duration_mins < 10 THEN '2) 5 to 10 Mins'\n",
    "            WHEN duration_mins >= 10 AND duration_mins < 20 THEN '3) 10 to 20 Mins'\n",
    "            WHEN duration_mins >= 20 AND duration_mins < 30 THEN '4) 20 to 30 Mins'\n",
    "            WHEN duration_mins >= 30 AND duration_mins < 60 THEN '5) 30 to 60 Mins'\n",
    "            ELSE '6) Over 60 Mins'\n",
    "        END AS bins_duration,\n",
    "        total_amount,\n",
    "        trip_distance,\n",
    "        duration_hours\n",
    "    FROM taxi_trips\n",
    ") AS sub_query\n",
    "GROUP BY\n",
    "    bins_duration\n",
    "ORDER BY\n",
    "    bins_duration;\n",
    "\n",
    "\"\"\")\n",
    "display(answer_7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7b7ebb3-6010-4c08-bcb7-72d6b9947134",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Answer 7: \n",
    "As the result in the calculation, Over 60 min and 30 to 60 mins duration seem have the high paid but when considering a time spent in a vehicle is too long compared to the number of trips that less than any bin duration which come with long time waiting. For under 5 min trip, even they have less time spent, the fare is very low. So, the best bin to maximise the paid, it is 10 to 20 mins bin because they have less distance per dollar with so massive trips and time spent close to the over 60 mins bin.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95538698-50d9-4d96-949e-fa54a533a5e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bde-assignment-2-part-2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
